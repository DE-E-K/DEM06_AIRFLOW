# Bangladesh Flight Price Pipeline

![Status](https://img.shields.io/badge/Status-Active-success)
![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![Airflow](https://img.shields.io/badge/Airflow-2.7.3-orange)
![License](https://img.shields.io/badge/License-MIT-green)

An Airflow data engineering pipeline designed to ingest, validate, transform, and analyze flight price data for the Bangladeshi aviation market. Orchestrated with **Apache Airflow**, this solution processes raw flight data to generate actionable insights on fare trends, seasonal variations, and route popularity.

## Key Features

- **Automated Ingestion**: Seamlessly loads raw CSV execution data into a MySQL staging environment.
- **Robust Validation**: Implements clear data quality checks (schema, types, nulls, business rules) with audit trails.
- **Advanced Analytics**: Computes complex KPIs including:
    - **Airline Pricing Strategy**: Average fares and tax breakdowns by airline.
    - **Seasonal Analysis**: Price fluctuations during peak (Eid, Winter) vs. off-peak seasons.
    - **Route Intelligence**: Identification of most popular and profitable routes.
- **Scalable Architecture**: Containerized with Docker Compose for consistent deployment across environments.

## ðŸ—ï¸ Architecture

The pipeline follows a modern ELT (Extract, Load, Transform) pattern:

```mermaid
graph LR
    A[Raw CSV] -->|Ingest| B[(MySQL Staging)]
    B -->|Validate| C{Data Quality}
    C -->|Valid| D[Transformation]
    C -->|Invalid| E[Error Log]
    D -->|Enrich| F[(PostgreSQL Analytics)]
    F -->|Query| G[KPI Dashboards]
```

For a detailed technical deep-dive, please refer to the [Architecture Documentation](docs/ARCHITECTURE.md).

## Project Structure

```
flight-price-pipeline/
â”œâ”€â”€ dags/                   # Airflow DAG definitions
â”œâ”€â”€ plugins/                # Custom Airflow operators and sensors
â”œâ”€â”€ src/                    # Core application logic (ETL, Validation)
â”œâ”€â”€ sql/                    # Database initialization and KPI queries
â”œâ”€â”€ docker/                 # Container orchestration configurations
â”œâ”€â”€ docs/                   # Detailed project documentation
â”œâ”€â”€ config/                 # Environment variables and app config
â”œâ”€â”€ data/                   # Input datasets
â””â”€â”€ logs/                   # Logs and reports
```

## Quick Start

### Prerequisites
- **Docker Desktop 29.1** 
- **Python 3.11** (util for key generation)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/DE-E-K/DEM06_AIRFLOW.git
   cd DEM06_AIRFLOW
   ```

2. **Setup Configuration**
   Generate a Fernet key for Airflow security:
   ```bash
   python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
   ```
   Update `config/.env` with the generated key and your secure credentials.

3. **Launch the Pipeline**
   ```bash
   docker compose --env-file config/.env up --build -d
   ```

4. **Access Interfaces**
   - **Airflow UI**: [http://localhost:8090](http://localhost:8090) (Default from environment: `admin` / `admin_password`)
   - **MySQL Staging**: `localhost:3308`
   - **PostgreSQL Analytics**: `localhost:5435`

## Pipeline Report Output

- A JSON execution report is generated by the `generate_report` task for each DAG run.
- Report files are written to `logs/reports/` in the project workspace.
- File name pattern: `flight_pipeline_report_<run_id>.json`.

## Documentation

- [Architecture Overview](docs/ARCHITECTURE.md) - System design and technology choices.
- [Setup Guide](docs/SETUP_GUIDE.md) - Detailed installation and troubleshooting.
- [DAG Explanations](docs/DAG_DOCUMENTATION.md) - Workflow logic and task breakdown.
- [KPI Definitions](docs/KPI_DEFINITIONS.md) - Business logic behind the metrics.
- [Database Interaction Guide](docs/DATABASE_INTERACTION.md) - How to connect, query, and validate data in MySQL and PostgreSQL.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
